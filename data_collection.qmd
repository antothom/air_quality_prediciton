---
title: "Data Collection"
execute:
  message: false
  warning: false
author: "Antoine Thomas"
date: 2023-08-03
format:
  html: 
    embed-resources: true
    code-tools:
      source: true
      toggle: true
      caption: "Show code"
editor: source
theme:
  light: flatly
  dark: darkly
---

```{r Loading packages}
#| include: false
library(tidyverse)
library(httr)
library(jsonlite)
library(lubridate)
library(furrr)
library(RColorBrewer)
library(ggmap)
```

Data collection involves identifying and consolidating the data sources required. For air quality (AQ) prediction, historical data on air pollutants are the most important. In addition, coherent data can be brought in. AQ is known to be strongly influenced by the local weather, but also by factors such as traffic intensity, local road construction sites, ongoing construction projects or industrial enterprises operating in the surrounding area. AQ monitoring data, being particulate matter (PM~2.5~/PM~10~), ozone (O~3~) and nitrogen dioxide (NO~2~), from the city of Berlin and weather data from the open source provider *open-meteo.com* were selected as primary data sources. In addition, data from the Berlin traffic detection (*Berlin Open Data*) were included.

### Reading in air quality monitoring data

```{r Air quality monitoring data}

# Reading in the master data for air quality measurement stations
airquality_stations <- GET("https://luftdaten.berlin.de/api/stations") %>% 
  parse_json(simplifyVector = T) %>% 
  as_tibble() %>%
  mutate(lat = as.numeric(lat),
         lng = as.numeric(lng),
         stationgroups = unlist(stationgroups),
         components = as.character(components),
         activeComponents = as.character(activeComponents),
         partials = unlist(partials),
         lqis = as.character(lqis),
         exceeds = as.character(exceeds))

# Reading in the names/paths of the files to be imported
files_pm25 <- list.files(path = "Daten/Luftguetemessung/Feinstaub_PM2")
files_pm10 <- list.files(path = "Daten/Luftguetemessung/Feinstaub_PM10")
files_O3 <- list.files(path = "Daten/Luftguetemessung/Ozon_O3")
files_NO2 <- list.files(path = "Daten/Luftguetemessung/Stickstoffdioxid_NO2")

# Function to read in monitoring data of a single pollutant data set
read_pollutants <- function(x, folder) {
  read_csv2(file = paste0("Daten/Luftguetemessung/", folder, "/", x)) %>%
    .[-(1:3),] %>%
    mutate(Station = dmy_hm(Station))
}

# Reading in data monitoring PM2.5 
values_pm25 <- sapply(files_pm25, read_pollutants, folder = "Feinstaub_PM2") %>%
  bind_rows() %>%
  unique() %>%
  rename("date" = Station) %>%
  gather(key = "Station", value = "pm25", 2:15)

# Reading in data monitoring PM10
values_pm10 <- sapply(files_pm10, read_pollutants, folder = "Feinstaub_PM10") %>%
  bind_rows() %>%
  unique() %>%
  rename("date" = Station) %>%
  gather(key = "Station", value = "pm10", 2:15)

# Reading in data monitoring O3
values_O3 <- sapply(files_O3, read_pollutants, folder = "Ozon_O3") %>%
  bind_rows() %>%
  unique() %>%
  rename("date" = Station) %>%
  gather(key = "Station", value = "O3", 2:11)

# Reading in data monitoring NO2
values_NO2 <- sapply(files_NO2, read_pollutants, folder = "Stickstoffdioxid_NO2") %>%
  bind_rows() %>%
  unique() %>%
  rename("date" = Station) %>%
  gather(key = "Station", value = "NO2", 2:21)


## Gathering all air quality monitoring data  
airquality_df <- purrr::reduce(list(values_pm25,
                                values_pm10,
                                values_O3,
                                values_NO2), dplyr::full_join, by = c("date", "Station")) %>%
  mutate(pm25 = as.numeric(pm25),
         pm10 = as.numeric(pm10),
         O3 = as.numeric(O3),
         NO2 = as.numeric(NO2))

## Removing not needed data and functions
remove(values_pm25,
       values_pm10,
       values_O3,
       values_NO2,
       files_NO2,
       files_O3,
       files_pm10,
       files_pm25,
       read_pollutants)

```

After the import process, a data set `airquality_df` with AQ data for the period from January 2016 to April 2023 is available. In addition, a master data set `monitoring_stations` is available, which contains basic and general information about the various monitoring stations in Berlin.

```{r glimpse air quality data set}
glimpse(airquality_df)
glimpse(airquality_stations)
```

### Reading in weather data

```{r Weather data}
# Variables to import:
# - hourly: temperature, humidity, dewpoint, surface pressure, precipitation, 
# windspeed, winddirection
# - daily: sunrise time, sunset time
# Timeframe: 01.01.2016 - 30.05.2023
weather_df_hourly <- GET("https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2016-01-01&end_date=2023-05-30&hourly=temperature_2m,relativehumidity_2m,dewpoint_2m,surface_pressure,precipitation,windspeed_100m,winddirection_100m&timezone=Europe%2FBerlin&windspeed_unit=ms") %>% 
  parse_json(simplifyVector = T)

# Reading in daily weather data
weather_df_daily <- GET("https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2016-01-01&end_date=2023-05-30&daily=sunrise,sunset&timezone=Europe%2FBerlin") %>% 
  parse_json(simplifyVector = T)

# Wrangling data from the API to a consistent data frame
weather_df_hourly$hourly_units <- names(weather_df_hourly$hourly_units)
weather_df_hourly <- weather_df_hourly %>%  
  as_tibble() %>%
  spread(hourly_units, hourly) %>% # change df to long format 
  unnest(cols = names(.)[7:length(names(.))]) %>% # unnest weather data variables
  relocate(time, .after = elevation) %>%
  mutate(time = ymd_hm(time)) %>%
  select(-latitude, -longitude, -utc_offset_seconds, -generationtime_ms, -timezone, -timezone_abbreviation, -elevation)

weather_df_daily$daily_units <- names(weather_df_daily$daily_units)
weather_df_daily <- weather_df_daily %>%  
  as_tibble() %>%
  spread(daily_units, daily) %>% # change df to long format 
  unnest(cols = names(.)[7:length(names(.))]) %>% # unnest weather data variables
  relocate(time, .after = elevation) %>%
  mutate(time = ymd(time),
         sunrise = ymd_hm(sunrise),
         sunset = ymd_hm(sunset),
         # Compute duration of sunlight on a specific day based on sunrise and sunset times
         duration_sunlight = as.numeric(difftime(sunset, sunrise, units="mins"))) %>%
  select(time, duration_sunlight)

# Gathering hourly and daily weather data
weather_df <- weather_df_hourly %>%
  mutate(date = floor_date(time, unit = "days")) %>%
  left_join(weather_df_daily, by = c("date" = "time")) %>%
  select(-date)

# Removing not needed data
remove(weather_df_daily,
       weather_df_hourly)
```

After the import process, a data set `weather_df` with hourly and daily weather data for the period from January 2016 to May 2023 is available.

```{r glimpse weather data set}
glimpse(weather_df)
```

### Reading in traffic data

```{r Traffic Data}
# Reading in traffic detector master data
traffic_detectors <- readxl::read_xlsx("Daten/Verkehrsdaten/Stammdaten_Verkehrsdetektion.xlsx") %>%
  select(MQ_KURZNAME, # mq = Messquerschnitt
         STRASSE,
         POSITION,
         POS_DETAIL,
         RICHTUNG,
         `LÄNGE (WGS84)`,
         `BREITE (WGS84)`,
         INBETRIEBNAHME,
         DEINSTALLIERT,
         KOMMENTAR) %>%
  rename("cs_shortname" = MQ_KURZNAME, #cs = cross-section
         "street" = STRASSE,
         "position" = POSITION,
         "position_detail" = POS_DETAIL,
         "direction" = RICHTUNG,
         "lng" = `LÄNGE (WGS84)`,
         "lat" = `BREITE (WGS84)`,
         "launch_date" = INBETRIEBNAHME,
         "uninstallment_date" = DEINSTALLIERT,
         "comment" = KOMMENTAR) %>%
  unique()


plan(multisession, workers = 4)
# Create a dataframe for each month and year from January 2017 to April 2023
traffic_df <- expand.grid(c(01:12),c(2017:2023)) %>%
  as_tibble() %>%
  filter(!(Var1 > 4 & Var2 == 2023)) %>%
  mutate(Var1 = ifelse(Var1 < 10, paste0("0",Var1), Var1),
         # Mutating all file URLs - File names tend to be slightly different
         # depending on the year.
         file_url = paste0("https://mdhopendata.blob.core.windows.net/verkehrsdetektion/", Var2, "/Messquerschnitt%20(fahrtrichtungsbezogen)/mq_hr_", Var2, "_", Var1, ".csv.gz")) %>%
  mutate(file_url = ifelse(Var2 == 2021, str_replace(file_url, "%20\\(fahrtrichtungsbezogen\\)", ""),file_url),
         file_url = ifelse(Var2 == 2023, str_replace(file_url, "Messquerschnitt", "Messquerschnitte"),file_url),
         data = future_map(.x = file_url, .f = read_csv2)) %>%
  pull(data) %>%
  bind_rows() %>%
  mutate(date = as.Date(tag, format = "%d.%m.%Y") + hours(stunde)) %>%
  relocate(date, .after = mq_name) %>%
  select(-tag, -stunde) %>%
  rename("cs_shortname" = mq_name,
         "quality" = qualitaet)
  
```

After the import process, a data set `traffic_df` with hourly traffic data for the period from January 2017 to April 2023 is available. In addition, a master data set `traffic_detectors` is available, which contains basic and general information about the various traffic detection sensors in Berlin.

```{r glimpse traffic data}
glimpse(traffic_df)
glimpse(traffic_detectors)
```

### Gathering and exporting data

In order to make effective use of the imported data, it is necessary to combine and merge individual data sets. In particular, weather data must be combined with air quality data because they will always be used together in further analysis. Traffic data is not combined with air quality or weather data in the data collection step. The first step is to identify individual air quality monitoring stations that could be considered for such a link.

```{r Gathering/Exporting data}

# Joining air quality data with weather data
air_weather_df <- airquality_df %>%
  left_join(weather_df, by = c("date" = "time")) %>%
  left_join(airquality_stations %>% select(name, stationgroups), by = c("Station" = "name")) %>%
  relocate(stationgroups, .after = Station)


# Exporting the combined dataset as .csv  
write.csv2(air_weather_df, file = "Daten/DataCollection/air_weather_df.csv")

# Exporting other data sets as .csv
write.csv2(traffic_df, file = "Daten/DataCollection/traffic_df.csv")
write.csv2(traffic_detectors, file = "Daten/DataCollection/traffic_detectors.csv")
write.csv2(airquality_stations, file = "Daten/DataCollection/airquality_stations.csv")
```

After the gathering process, a data set `air_weather_df` combining air quality measurement data and weather data is available. This data set is exported as .csv for usage within the following analysis. The previously collected data such as `traffic_df`, `traffic_detectors` and `airquality_stations` are exported as well.

### Session info

```{r Session info}
#| echo: false
sessionInfo()
```
